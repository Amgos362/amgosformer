{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amgos45/coin_transformer/amgos/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/tmp/ipykernel_1080466/1019347448.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_experiment_15.pth\", map_location=device))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EncoderOnlyTransformerCustom:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1, 512]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 135\u001b[0m\n\u001b[1;32m    129\u001b[0m model \u001b[38;5;241m=\u001b[39m EncoderOnlyTransformerCustom(\n\u001b[1;32m    130\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39minput_dim, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, nhead\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m    131\u001b[0m     ffn_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_seq_len\u001b[38;5;241m=\u001b[39mlookback\n\u001b[1;32m    132\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# 학습된 파라미터 로드\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_experiment_15.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# 3. 백테스팅 실행 및 예측값 저장\u001b[39;00m\n",
      "File \u001b[0;32m~/coin_transformer/amgos/lib/python3.8/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EncoderOnlyTransformerCustom:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1, 512]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2])."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# rolling minmax scaling 함수 (window=24)\n",
    "def rolling_minmax_scale(series, window=24):\n",
    "    roll_min = series.rolling(window=window, min_periods=window).min()\n",
    "    roll_max = series.rolling(window=window, min_periods=window).max()\n",
    "    scaled = (series - roll_min) / ((roll_max - roll_min) + 1e-8)\n",
    "    scaled = scaled.replace([np.inf, -np.inf], np.nan)\n",
    "    scaled = scaled.fillna(1.0)\n",
    "    return scaled.clip(upper=1.0)\n",
    "\n",
    "# binning 및 one-hot 인코딩 함수 (결과를 정수 0,1로)\n",
    "def bin_and_encode(data, features, bins=100, drop_original=True):\n",
    "    for feature in features:\n",
    "        data[f'{feature}_Bin'] = pd.cut(data[feature], bins=bins, labels=False)\n",
    "        one_hot = pd.get_dummies(data[f'{feature}_Bin'], prefix=f'{feature}_Bin').astype(np.int32)\n",
    "        expected_columns = [f'{feature}_Bin_{i}' for i in range(bins)]\n",
    "        one_hot = one_hot.reindex(columns=expected_columns, fill_value=0)\n",
    "        data = pd.concat([data, one_hot], axis=1)\n",
    "        if drop_original:\n",
    "            data.drop(columns=[f'{feature}_Bin'], inplace=True)\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        data[col] = data[col].astype('float32')\n",
    "    return data\n",
    "\n",
    "# TimeSeriesDataset (분류용): lookback 후, 바로 다음 봉의 close 값을 이전 봉과 비교하여 상승이면 1, 하락이면 0을 타깃으로 함\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, lookback=24):\n",
    "        self.input_data = input_data.values\n",
    "        self.target_data = target_data.values\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data) - self.lookback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_data[idx: idx + self.lookback, :]\n",
    "        # 타깃: lookback 후의 close 값과 바로 전 봉(close 값) 비교\n",
    "        y = self.target_data[idx + self.lookback, 0]\n",
    "        y_prev = self.target_data[idx + self.lookback - 1, 0]\n",
    "        y_target = 1 if y > y_prev else 0\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y_target, dtype=torch.long)\n",
    "\n",
    "# 분류 모델 정의 (상승/하락 예측)\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_layers=6, nhead=8, \n",
    "                 ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=nhead, dim_feedforward=ffn_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        x = x + pos_emb\n",
    "        x = x.transpose(0, 1)  # Transformer의 입력형태: [seq_len, batch, features]\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.fc(x[-1, :, :])  # 마지막 타임스탭 출력\n",
    "\n",
    "# 평가 함수 (분류용)\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "# 1. 데이터 로드 및 전처리 (파일명 및 경로에 맞게 수정)\n",
    "data = pd.read_csv(\"ETH_upbit_KRW_min5.csv\", index_col=0)\n",
    "data = data[['open', 'high', 'low', 'close']]\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# 각 OHLC 열에 대해 rolling minmax scaling (window=24)\n",
    "ohlc_features = ['open', 'high', 'low', 'close']\n",
    "for feature in ohlc_features:\n",
    "    data[feature] = rolling_minmax_scale(data[feature], window=24)\n",
    "\n",
    "# one-hot 인코딩: OHLC 열은 100 구간으로 나눔\n",
    "data_encoded = bin_and_encode(data.copy(), ohlc_features, bins=100, drop_original=True)\n",
    "\n",
    "# 타깃은 원본 close 값을 사용 (상승/하락 판단용)\n",
    "data['close_target'] = data['close']\n",
    "data = data.dropna()\n",
    "\n",
    "final_input_columns = [col for col in data_encoded.columns if '_Bin_' in col]\n",
    "final_target_column = ['close_target']\n",
    "\n",
    "data_input = data_encoded[final_input_columns]\n",
    "data_target = data[final_target_column]\n",
    "\n",
    "# 1. 백테스팅을 위한 데이터셋 구성\n",
    "lookback = 24\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 기존 데이터 전처리 유지\n",
    "data_encoded = bin_and_encode(data.copy(), ohlc_features, bins=100, drop_original=True)\n",
    "\n",
    "# target 복원 (data_encoded에 close_target을 추가)\n",
    "data_encoded['close_target'] = data['close']\n",
    "\n",
    "# 데이터셋 생성\n",
    "final_input_columns = [col for col in data_encoded.columns if '_Bin_' in col]\n",
    "data_input = data_encoded[final_input_columns]\n",
    "data_target = data_encoded[['close_target']]\n",
    "\n",
    "backtest_dataset = TimeSeriesDataset(data_input, data_target, lookback=lookback)\n",
    "backtest_loader = DataLoader(backtest_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 2. 모델 로드\n",
    "input_dim = data_input.shape[1]\n",
    "model = EncoderOnlyTransformerCustom(\n",
    "    input_dim=input_dim, embedding_dim=512, num_layers=6, nhead=8,\n",
    "    ffn_dim=2048, num_classes=2, max_seq_len=lookback\n",
    ").to(device)\n",
    "\n",
    "# 학습된 파라미터 로드\n",
    "model.load_state_dict(torch.load(\"model_experiment_15.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 3. 백테스팅 실행 및 예측값 저장\n",
    "predictions = []\n",
    "actuals = []\n",
    "timestamps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(backtest_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outputs = model(x)\n",
    "        probs = torch.softmax(outputs, dim=1)  # 예측 확률 계산\n",
    "        predicted = torch.argmax(probs, dim=1)  # 예측 클래스 (0: 하락, 1: 상승)\n",
    "\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        actuals.extend(y.cpu().numpy())\n",
    "        timestamps.extend(data.index[lookback + i * 32:lookback + (i + 1) * 32])  # 타임스탬프 저장\n",
    "\n",
    "# 4. 결과 DataFrame 생성\n",
    "backtest_results = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'actual': actuals,\n",
    "    'predicted': predictions\n",
    "})\n",
    "\n",
    "# 예측 정확도 계산\n",
    "accuracy = (backtest_results['actual'] == backtest_results['predicted']).mean()\n",
    "print(f\"Backtest Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# 5. CSV로 저장\n",
    "backtest_results.to_csv(\"backtest_results.csv\", index=False)\n",
    "print(\"백테스팅 결과가 backtest_results.csv 파일로 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amgos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
