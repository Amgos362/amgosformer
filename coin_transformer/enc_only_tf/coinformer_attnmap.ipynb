{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sqrt\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m####################################\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 커스텀 Transformer Encoder Layer (어텐션 가중치 반환)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m####################################\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "####################################\n",
    "# 커스텀 Transformer Encoder Layer (어텐션 가중치 반환)\n",
    "####################################\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU() if activation==\"relu\" else nn.GELU()\n",
    "        \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # need_weights=True로 어텐션 가중치를 반환합니다.\n",
    "        attn_output, attn_weights = self.self_attn(\n",
    "            src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask, need_weights=True)\n",
    "        src2 = self.dropout1(attn_output)\n",
    "        src = self.norm1(src + src2)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = self.norm2(src + src2)\n",
    "        return src, attn_weights\n",
    "\n",
    "####################################\n",
    "# 커스텀 Transformer Encoder (여러 레이어)\n",
    "####################################\n",
    "class CustomTransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        attn_weights_all = []\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output, attn_weights = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "            attn_weights_all.append(attn_weights)  # 각 레이어의 어텐션 가중치 저장\n",
    "        return output, attn_weights_all\n",
    "\n",
    "####################################\n",
    "# 기존 전처리, Dataset 등은 그대로 사용 (생략)\n",
    "####################################\n",
    "# (여기서는 기존 코드의 rolling_minmax_scale, bin_and_encode, TimeSeriesDataset 등 그대로 둡니다.)\n",
    "\n",
    "####################################\n",
    "# 모델 정의 (커스텀 Encoder 사용)\n",
    "####################################\n",
    "class EncoderOnlyTransformerCustom(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=512, num_layers=6, nhead=8,\n",
    "                 ffn_dim=2048, num_classes=2, max_seq_len=24):\n",
    "        super(EncoderOnlyTransformerCustom, self).__init__()\n",
    "        self.token_embedding = nn.Linear(input_dim, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        # 커스텀 Transformer Encoder 사용\n",
    "        encoder_layer = CustomTransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dim_feedforward=ffn_dim)\n",
    "        self.transformer_encoder = CustomTransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        # forward 실행 후 어텐션 가중치를 저장할 변수\n",
    "        self.last_attn_weights = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = self.token_embedding(x)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        x = x + pos_emb\n",
    "        x = x.transpose(0, 1)  # [seq_len, batch, embedding_dim]\n",
    "        output, attn_weights_all = self.transformer_encoder(x)\n",
    "        self.last_attn_weights = attn_weights_all  # 전체 레이어의 어텐션 가중치 저장\n",
    "        return self.fc(output[-1, :, :])  # 마지막 타임스탭의 출력으로 분류 예측\n",
    "\n",
    "####################################\n",
    "# 어텐션 맵 시각화 함수\n",
    "####################################\n",
    "def plot_attention_map(attn_map, head_idx=0):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attn_map[0, head_idx], cmap=\"viridis\", annot=False)\n",
    "    plt.xlabel(\"Key\")\n",
    "    plt.ylabel(\"Query\")\n",
    "    plt.title(f\"Attention Map (Head {head_idx})\")\n",
    "    plt.show()\n",
    "\n",
    "####################################\n",
    "# 예시: 모델 실행 후 어텐션 가중치 확인하기\n",
    "####################################\n",
    "# 예를 들어, 백테스팅용 DataLoader에서 한 배치를 가져와 모델에 넣고 어텐션 가중치를 출력해봅니다.\n",
    "# (아래는 단독 실행 예시이며, 실제 코드에 맞게 DataLoader나 입력 데이터를 구성해야 합니다.)\n",
    "\n",
    "# 가상의 입력 (batch_size=2, seq_len=24, input_dim=400) 생성\n",
    "dummy_input = torch.randn(2, 24, 400)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dummy_input = dummy_input.to(device)\n",
    "\n",
    "# 모델 생성 (입력 차원 400, 분류 2개)\n",
    "model = EncoderOnlyTransformerCustom(input_dim=400).to(device)\n",
    "model.eval()\n",
    "\n",
    "# forward 실행\n",
    "with torch.no_grad():\n",
    "    outputs = model(dummy_input)\n",
    "    # 이제 model.last_attn_weights는 리스트로 각 레이어의 어텐션 가중치를 포함합니다.\n",
    "    # 예: 첫 번째 레이어의 어텐션 가중치 확인 (shape: [batch, num_heads, seq_len, seq_len])\n",
    "    first_layer_attn = model.last_attn_weights[0]\n",
    "    print(\"첫 번째 레이어 어텐션 가중치 shape:\", first_layer_attn.shape)\n",
    "    # 시각화 (예: 첫 번째 헤드)\n",
    "    plot_attention_map(first_layer_attn, head_idx=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amgos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
